training:
  model: "google/gemma-3-270m-it"   # model checkpoint to load
  output_dir: "./checkpoints"       # directory to save checkpoints
  max_length: 256                   # Reduced from 512 to save memory
  packing: false                    # whether to pack multiple samples per sequence
  num_train_epochs: 2
  per_device_train_batch_size: 1    # Keep small for memory efficiency
  gradient_checkpointing: true      # Enable to save memory
  optim: "adamw_torch"              # Avoid fused optimizers for memory
  logging_steps: 1
  save_strategy: "epoch"
  eval_strategy: "epoch"
  learning_rate: 6e-5               # Conservative learning rate
  fp16: true                        # Use fp16 to save memory
  bf16: false                       # Use fp16 instead
  lr_scheduler_type: "constant"
  push_to_hub: false                # Disable during development
  report_to: "none"                 # Disable reporting to save memory
  fsdp: ""                          # Disable FSDP for single GPU

dataset:
  data_path: "./data/train.csv"
  test_size: 0.2
  dataset_kwargs:
    add_special_tokens: false
    append_concat_token: true

# Simplified hyperparameter space for memory constraints
hyperparameter_space:
  # Learning rate optimization (reduced range)
  learning_rate:
    type: "float"
    min: 1e-6
    max: 1e-4
    log: true
  
  # Optimizer selection (memory-efficient options only)
  optimizer:
    type: "categorical"
    choices: ["adamw_torch", "sgd", "adafactor"]

  per_device_train_batch_size:
    low: 1
    high: 3
  
  num_train_epochs:
    choices: [1, 2, 4]
  
# Reduced Optuna configuration for memory constraints
optuna:
  n_trials: 3                       # Reduced from 6
  direction: "minimize"
  sampler: "GPSampler"