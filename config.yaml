training:
  model: "google/gemma-3-270m-it"
  output_dir: "./results"
  max_length: 512
  packing: true
  num_train_epochs: 3
  per_device_train_batch_size: 2
  gradient_checkpointing: true
  optim: "adamw_torch"
  logging_steps: 1
  save_strategy: "epoch"
  eval_strategy: "epoch"
  learning_rate: 1e-6
  fp16: false 
  bf16: true  
  lr_scheduler_type: "cosine"
  push_to_hub: false
  report_to: "none" 
  fsdp: ""

lm_harness_evaluation:
  num_fewshot: 0          
  batch_size: 1      
  limit: 1
  apply_chat_template: true
  fewshot_as_multiturn: false  


dataset:
  data_train_path: "/home/ubuntu/finetune/data/hyde/train.json"
  data_test_path: "/home/ubuntu/finetune/data/hyde/test.json"
  data_valid_path: "/home/ubuntu/finetune/data/hyde/valid.json"
  dataset_kwargs:
    trust_remote_code: false

# Hyperparameter search space
hyperparameter_space:
  learning_rate:
    min: 1e-6
    max: 1e-4
    log: true
  optimizer:
    choices: ["adamw_torch", "adafactor"]
  num_train_epoch:
    min: 1
    max: 3
  per_device_train_batch_size:
    choices: [1, 2]  # Keep small for memory constraints

# Optuna specific settings
optuna:
  n_trials: 12