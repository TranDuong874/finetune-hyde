training:
  model: "google/gemma-3-270m-it"   # model checkpoint to load
  output_dir: "./checkpoints"       # directory to save checkpoints
  max_length: 512                   # max sequence length
  packing: false                    # whether to pack multiple samples per sequence
  num_train_epochs: 2
  per_device_train_batch_size: 1
  gradient_checkpointing: false
  optim: "sgd"
  logging_steps: 1
  save_strategy: "epoch"
  eval_strategy: "epoch"
  learning_rate: 6e-5               # example value, set yours
  fp16: true                        # depends on model.dtype
  bf16: false                       # depends on model.dtype
  lr_scheduler_type: "constant"
  push_to_hub: true
  report_to: "tensorboard"
  fsdp: "full_shard auto_wrap"

dataset:
  data_path: "./data/train.csv"
  test_size: 0.2
  dataset_kwargs:
    add_special_tokens: false
    append_concat_token: true
