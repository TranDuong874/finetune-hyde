# config.yaml - Memory optimized configuration
training:
  model: "google/gemma-2-2b-it"  # Smaller model for limited memory
  output_dir: "./results"
  max_length: 512  # Reduced from potentially larger values
  packing: true
  num_train_epochs: 3
  per_device_train_batch_size: 2  # Small batch size
  gradient_checkpointing: true
  optim: "adamw_torch"
  logging_steps: 10
  save_strategy: "epoch"
  eval_strategy: "epoch"
  learning_rate: 2e-5
  fp16: true  # Use half precision
  bf16: false  # Use fp16 OR bf16, not both
  lr_scheduler_type: "cosine"
  push_to_hub: false
  report_to: "none" 
  fsdp: ""

dataset:
  data_path: "your_data.csv"
  dataset_kwargs:
    trust_remote_code: false

# Hyperparameter search space
hyperparameter_space:
  learning_rate:
    min: 1e-6
    max: 1e-4
    log: true
  optimizer:
    choices: ["adamw_torch", "adamw_hf"]
  num_train_epoch:
    min: 1
    max: 3
  per_device_train_batch_size:
    choices: [1, 2, 4]  # Keep small for memory constraints

# Optuna specific settings
optuna:
  n_trials: 6
  use_subset: true  # Use smaller dataset for hyperparameter search
  subset_size: 1000  # Number of samples for HP search