training:
  model: "google/gemma-3-270m-it"
  output_dir: "./results"
  max_length: 512
  packing: true
  num_train_epochs: 3
  per_device_train_batch_size: 2
  gradient_checkpointing: true
  optim: "adamw_torch"
  logging_steps: 1
  save_strategy: "epoch"
  eval_strategy: "epoch"
  learning_rate: 1e-6
  fp16: false 
  bf16: true  
  lr_scheduler_type: "cosine"
  push_to_hub: false
  report_to: "none" 
  fsdp: ""

dataset:
  data_path: "/kaggle/input/hyde-finetuning-vchatbot/final_cq.csv"
  dataset_kwargs:
    trust_remote_code: false

# Hyperparameter search space
hyperparameter_space:
  learning_rate:
    min: 1e-6
    max: 1e-5
    log: true
  optimizer:
    choices: ["adamw_torch", "sgd", "adafactor"]
  num_train_epoch:
    min: 1
    max: 3
  per_device_train_batch_size:
    choices: [1, 2]  # Keep small for memory constraints

# Optuna specific settings
optuna:
  n_trials: 6
  use_subset: false  
  subset_size: 1000